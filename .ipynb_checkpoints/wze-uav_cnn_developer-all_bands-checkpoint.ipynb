{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1acdaa3",
   "metadata": {},
   "source": [
    "# WZE-UAV Image Classification using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2159aac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20cade3b8b0>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import mlxtend\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "x = (1,3)\n",
    "y = (1,3)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0026f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b160d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wze_uav.data_loader as data_loader\n",
    "import wze_uav.visualization as visualization\n",
    "import wze_uav.models as models\n",
    "from wze_uav.engine import *\n",
    "from wze_uav.utils2 import *\n",
    "from wze_uav.log_writer import create_writer\n",
    "from efficientnet import model #for custom effnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4fb2f",
   "metadata": {},
   "source": [
    "#### Get PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10886b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.13.1+cu116\n",
      "torchvision version: 0.14.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a83cf",
   "metadata": {},
   "source": [
    "#### Preparing device agnostic code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d5de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Index of current divice: 0\n",
      "Number of GPUs available: 1\n",
      "GPU Model: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "# ensure device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# get index of currently selected device\n",
    "print(f\"Index of current divice: {torch.cuda.current_device()}\")\n",
    "# get number of GPUs available\n",
    "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "# get the name of the device\n",
    "print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a6a34",
   "metadata": {},
   "source": [
    "#### Ensure reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4656a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for more information, see also: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Set seeds\n",
    "def set_seeds(seed: int=0):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 0.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# seed for numpy\n",
    "np.random.seed(0)\n",
    "# avoiding non-deterministic algorithms\n",
    "#torch.use_deterministic_algorithms(True)\n",
    "# Set to true -> might speed up the process but should be set to False if reproducible results are desired\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be33671",
   "metadata": {},
   "source": [
    "#### Define file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30645f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds() # ensure reproducibility\n",
    "\n",
    "# Data 2020\n",
    "train_data_path = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2020\\ROI\\gbrnir\\train\"\n",
    "val_data_path = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2020\\ROI\\gbrnir\\val\"\n",
    "test_data_path = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2020\\ROI\\gbrnir\\test\"\n",
    "\n",
    "# Data 2021\n",
    "train_data_path_2021 = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2021\\ROI\\gbrnir\\train\"\n",
    "val_data_path_2021 = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2021\\ROI\\gbrnir\\val\"\n",
    "test_data_path_2021 = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2021\\ROI\\gbrnir\\test\"\n",
    "\n",
    "# Data 2022\n",
    "train_data_path_2022 = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2022\\ROI\\gbrnir\\train\"\n",
    "val_data_path_2022 = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2022\\ROI\\gbrnir\\val\"\n",
    "test_data_path_2022 = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2022\\ROI\\gbrnir\\test\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d28343",
   "metadata": {},
   "source": [
    "#### Load images and features (labels and tree species) from hdf5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b72aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all image and feature arrays from hdf5\n",
    "# loops through all hdf5 files! \n",
    "\n",
    "# Data 2020\n",
    "train_image_set_2020, train_label_set_2020, train_species_set_2020, train_kkl_set_2020, train_bk_set_2020 = data_loader.hdf5_to_img_label(train_data_path, load_sets = [\"images_masked\"])\n",
    "val_image_set_2020, val_label_set_2020, val_species_set_2020, val_kkl_set_2020, val_bk_set_2020 = data_loader.hdf5_to_img_label(val_data_path, load_sets = [\"images_masked\"])\n",
    "test_image_set_2020, test_label_set_2020, test_species_set_2020, test_kkl_set_2020, test_bk_set_2020 = data_loader.hdf5_to_img_label(test_data_path, load_sets = [\"images_masked\"])\n",
    "\n",
    "# Data 2021\n",
    "train_image_set_2021, train_label_set_2021, train_species_set_2021, train_kkl_set_2021, train_bk_set_2021 = data_loader.hdf5_to_img_label(train_data_path_2021, load_sets = [\"images_masked\"])\n",
    "val_image_set_2021, val_label_set_2021, val_species_set_2021, val_kkl_set_2021, val_bk_set_2021 = data_loader.hdf5_to_img_label(val_data_path_2021, load_sets = [\"images_masked\"])\n",
    "test_image_set_2021, test_label_set_2021, test_species_set_2021, test_kkl_set_2021, test_bk_set_2021 = data_loader.hdf5_to_img_label(test_data_path_2021, load_sets = [\"images_masked\"])\n",
    "\n",
    "# Data 2022\n",
    "train_image_set_2022, train_label_set_2022, train_species_set_2022, train_kkl_set_2022, train_bk_set_2022 = data_loader.hdf5_to_img_label(train_data_path_2022, load_sets = [\"images_masked\"])\n",
    "val_image_set_2022, val_label_set_2022, val_species_set_2022, val_kkl_set_2022, val_bk_set_2022 = data_loader.hdf5_to_img_label(val_data_path_2022, load_sets = [\"images_masked\"])\n",
    "test_image_set_2022, test_label_set_2022, test_species_set_2022, test_kkl_set_2022, test_bk_set_2022 = data_loader.hdf5_to_img_label(test_data_path_2022, load_sets = [\"images_masked\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack all data from 2020, 2021 and 2022 into one dataset\n",
    "# images\n",
    "train_image_set = np.concatenate((train_image_set_2020, train_image_set_2021, train_image_set_2022), axis=0)\n",
    "val_image_set = np.concatenate((val_image_set_2020, val_image_set_2021, val_image_set_2022), axis=0)\n",
    "test_image_set = np.concatenate((test_image_set_2020, test_image_set_2021, test_image_set_2022), axis=0)\n",
    "\n",
    "#labels\n",
    "train_label_set = np.concatenate((train_label_set_2020, train_label_set_2021, train_label_set_2022), axis=0)\n",
    "val_label_set = np.concatenate((val_label_set_2020, val_label_set_2021, val_label_set_2022), axis=0)\n",
    "test_label_set = np.concatenate((test_label_set_2020, test_label_set_2021, test_label_set_2022), axis=0)\n",
    "\n",
    "#species\n",
    "train_species_set = np.concatenate((train_species_set_2020, train_species_set_2021, train_species_set_2022), axis=0)\n",
    "val_species_set = np.concatenate((val_species_set_2020, val_species_set_2021, val_species_set_2022), axis=0)\n",
    "test_species_set = np.concatenate((test_species_set_2020, test_species_set_2021, test_species_set_2022), axis=0)\n",
    "\n",
    "#kkl\n",
    "train_kkl_set = np.concatenate((train_kkl_set_2020, train_kkl_set_2021, train_kkl_set_2022), axis=0)\n",
    "val_kkl_set = np.concatenate((val_kkl_set_2020, val_kkl_set_2021, val_kkl_set_2022), axis=0)\n",
    "test_kkl_set = np.concatenate((test_kkl_set_2020, test_kkl_set_2021, test_kkl_set_2022), axis=0)\n",
    "\n",
    "#bk\n",
    "train_bk_set = np.concatenate((train_bk_set_2020, train_bk_set_2021, train_bk_set_2022), axis=0)\n",
    "val_bk_set = np.concatenate((val_bk_set_2020, val_bk_set_2021, val_bk_set_2022), axis=0)\n",
    "test_bk_set = np.concatenate((test_bk_set_2020, test_bk_set_2021, test_bk_set_2022), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shapes and amount of samples. Image sample size should be equal to label sample size!\n",
    "# Images should be of shape (n, H, W, C): n = sample size | H = image height | W = image width | C = number of bands\n",
    "print(\"Dataset 2020\")\n",
    "print(f\"\\nTrain dataset shape: {train_image_set_2020.shape}\")\n",
    "print(f\"Train labels shape: {train_label_set_2020.shape}\")\n",
    "print(f\"Train species shape: {train_species_set_2020.shape}\")\n",
    "print(f\"Train kkl shape: {train_kkl_set_2020.shape}\")\n",
    "print(f\"Train bk shape: {train_bk_set_2020.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Validation dataset shape: {val_image_set_2020.shape}\")\n",
    "print(f\"Validation labels shape: {val_label_set_2020.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Test dataset shape: {test_image_set_2020.shape}\")\n",
    "print(f\"Test labels shape: {test_label_set_2020.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nTotal image samples: {train_image_set_2020.shape[0] + val_image_set_2020.shape[0] + test_image_set_2020.shape[0]}\")\n",
    "print(f\"Total label samples: {train_label_set_2020.shape[0] + val_label_set_2020.shape[0] + test_label_set_2020.shape[0]}\\n\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nDataset 2021\")\n",
    "print(f\"\\nTrain dataset shape: {train_image_set_2021.shape}\")\n",
    "print(f\"Train labels shape: {train_label_set_2021.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Validation dataset shape: {val_image_set_2021.shape}\")\n",
    "print(f\"Validation labels shape: {val_label_set_2021.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Test dataset shape: {test_image_set_2021.shape}\")\n",
    "print(f\"Test labels shape: {test_label_set_2021.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nTotal image samples: {train_image_set_2021.shape[0] + val_image_set_2021.shape[0] + test_image_set_2021.shape[0]}\")\n",
    "print(f\"Total label samples: {train_label_set_2021.shape[0] + val_label_set_2021.shape[0] + test_label_set_2021.shape[0]}\\n\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nDataset 2022\")\n",
    "print(f\"\\nTrain dataset shape: {train_image_set_2022.shape}\")\n",
    "print(f\"Train labels shape: {train_label_set_2022.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Validation dataset shape: {val_image_set_2022.shape}\")\n",
    "print(f\"Validation labels shape: {val_label_set_2022.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Test dataset shape: {test_image_set_2022.shape}\")\n",
    "print(f\"Test labels shape: {test_label_set_2022.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nTotal image samples: {train_image_set_2022.shape[0] + val_image_set_2022.shape[0] + test_image_set_2022.shape[0]}\")\n",
    "print(f\"Total label samples: {train_label_set_2022.shape[0] + val_label_set_2022.shape[0] + test_label_set_2022.shape[0]}\\n\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nStacked Dataset\")\n",
    "print(f\"\\nTrain dataset shape: {train_image_set.shape}\")\n",
    "print(f\"Train labels shape: {train_label_set.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Validation dataset shape: {val_image_set.shape}\")\n",
    "print(f\"Validation labels shape: {val_label_set.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Test dataset shape: {test_image_set.shape}\")\n",
    "print(f\"Test labels shape: {test_label_set.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nTotal image samples: {train_image_set.shape[0] + val_image_set.shape[0] + test_image_set.shape[0]}\")\n",
    "print(f\"Total label samples: {train_label_set.shape[0] + val_label_set.shape[0] + test_label_set.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5104a",
   "metadata": {},
   "source": [
    "#### Define data transforms and create PyTorch Datasets (converts ndarrays into tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train transform with augmentation. \n",
    "transform_train = transforms.Compose([transforms.ToTensor(),\n",
    "                                      #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      #                     std=[0.229, 0.224, 0.225]),\n",
    "                                      #transforms.ColorJitter(brightness=(0.5,1.5), contrast=(0.5,1.2), saturation=(0.5,1.2), hue=(-0.05,0.05)),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.RandomVerticalFlip(p=0.5),\n",
    "                                      transforms.RandomRotation(degrees=[-90,90]),\n",
    "                                      #transforms.Resize(224)\n",
    "                                      ])\n",
    "\n",
    "# test and val dataset transform without augmentation. \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                #transforms.Resize(224)\n",
    "                               ])\n",
    "\n",
    "\n",
    "# class names need to fit the customDataset class used e.g. 3 classes -> use CustomDataset3Classes\n",
    "#class_names = ['healthy', 'slightly_stressed', 'moderately_stressed', 'highly_stressed', 'dead']\n",
    "#class_names = ['healthy', 'moderately_stressed', 'highly_stressed', 'dead']\n",
    "class_names = ['healthy', 'stressed', 'dead']\n",
    "\n",
    "# choose custom dataset loader with 3 - 5 classes\n",
    "train_dataset = data_loader.CustomDataset3Classes(\n",
    "    data = train_image_set,\n",
    "    labels = train_label_set,\n",
    "    class_names=class_names, \n",
    "    species = train_species_set,\n",
    "    kkl = train_kkl_set,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "val_dataset = data_loader.CustomDataset3Classes(\n",
    "    data = val_image_set,\n",
    "    labels = val_label_set,\n",
    "    class_names=class_names,\n",
    "    species = val_species_set,\n",
    "    kkl = val_kkl_set,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = data_loader.CustomDataset3Classes(\n",
    "    data = test_image_set,\n",
    "    labels = test_label_set,\n",
    "    class_names=class_names, \n",
    "    species = test_species_set,\n",
    "    kkl = test_kkl_set,\n",
    "    transform=transform\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550b86f",
   "metadata": {},
   "source": [
    "#### Visualize random images from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance brightness in colorjitter for better visibility\n",
    "visualization.display_random_images(train_dataset, \n",
    "                      n=2, \n",
    "                      classes=[0, 1, 2],\n",
    "                      seed=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04aca61",
   "metadata": {},
   "source": [
    "#### Check shape and data types of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052846b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[0]\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb1a7d",
   "metadata": {},
   "source": [
    "#### Create PyTorch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "NUM_WORKERS=3 # should be changed, depending on the system used\n",
    "batch_size=32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              persistent_workers=True,\n",
    "                              pin_memory=True,\n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              generator=g,\n",
    "                              shuffle=True,\n",
    "                              drop_last=False)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             persistent_workers=True,\n",
    "                             pin_memory=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             persistent_workers=True,\n",
    "                             pin_memory=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False)\n",
    "\n",
    "print(f\"Train dataloader size: {len(train_dataloader)}\")\n",
    "print(f\"Val dataloader size: {len(val_dataloader)}\")\n",
    "print(f\"Test dataloader size: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfb16e",
   "metadata": {},
   "source": [
    "#### Create and print model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "classes = train_dataset.class_names\n",
    "num_classes = len(classes) # get output_shape (number of classes) as an argument for the model\n",
    "n_bands = 4\n",
    "dropout_rate = 0.3\n",
    "\n",
    "effnet_b0_custom = model.EfficientNet.from_pretrained('efficientnet-b0', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out model summary\n",
    "summary(effnet_b0_custom, \n",
    "        input_size=(32, 4, 250, 250), # (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=17,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a94b5",
   "metadata": {},
   "source": [
    "#### Set model training parameters (multiple experiments possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create epochs list\n",
    "num_epochs = [5]\n",
    "\n",
    "# 2. Create models list (need to create a new model for each experiment)\n",
    "models_list = [\"effnet_b0\", \"effnet_b7\"]\n",
    "\n",
    "# 3. Create dataloaders dictionary for various dataloaders\n",
    "train_dataloaders = {\"train_data\": train_dataloader}\n",
    "\n",
    "# 4. Create target folder name were to save the tensorboard event files\n",
    "target_dir = \"log_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. set number of samples for each class to consider class imbalance\n",
    "\n",
    "#weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "#weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "\n",
    "\n",
    "#loss_weight = torch.Tensor([3730, 7632, 6578, 553, 312]) # 5 classes\n",
    "#loss_weight = torch.Tensor([11362, 6578, 553, 312]) # 4 classes\n",
    "\n",
    "\n",
    "# Loss weight for 3 classes\n",
    "#loss_weight = torch.Tensor([5.042, 1.274, 60.272]) # 3 classes | 0-10 = healthy, 15-95 = stressed, 99-100 = dead\n",
    "#loss_weight = torch.Tensor([1.655, 2.637, 60.272]) # 3 classes | 0-25 = healthy, 30-95 = stressed, 99-100 = dead\n",
    "#loss_weight = torch.Tensor([1.099, 12.732, 60.272]) # 3 classes| 0-45 = healthy, 50-95 = stressed, 99-100 = dead\n",
    "#num_samples_class = [11362, 7131, 312] # 3 classes| 0-25 = healthy, 30-95 = stressed, 99-100 = dead\n",
    "#num_samples_class = [16296, 2197, 312] # 3 classes| 0-40 = healthy, 45-95 = stressed, 99-100 = dead\n",
    "num_samples_class = [17016, 1477, 312] # 3 classes| 0-45 = healthy, 50-95 = stressed, 99-100 = dead\n",
    "\n",
    "loss_weight = [(1. / num_samples_class[i])*(18805 / 2.0) for i in range(len(num_samples_class))]\n",
    "loss_weight = torch.Tensor(loss_weight)\n",
    "print(f\"Loss weights: {loss_weight}\")\n",
    "loss_weight = loss_weight.to(device)\n",
    "# data 2020: [1194, 2516, 2776, 273, 97] (12 = NBV 99)\n",
    "# data 2021: [1097, 2548, 2475, 203, 112] (2 = NBV 99)\n",
    "# data 2022: [1439, 2568, 1327, 912, 88] (1 = NBV 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa4011",
   "metadata": {},
   "source": [
    "#### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c28092",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### define Parameters######\n",
    "classes = train_dataset.class_names\n",
    "num_classes = len(classes)\n",
    "n_bands = 4 # define number of bands\n",
    "lr = 0.001 # define learning rate\n",
    "dropout_rate = 0.25 #define dropout rate\n",
    "unfreeze = True\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c8494",
   "metadata": {},
   "source": [
    "#### Start model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Set the random seeds\n",
    "set_seeds(seed=0)\n",
    "\n",
    "# 2. Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# 4. Loop through each number of epochs\n",
    "for epochs in num_epochs:\n",
    "    # 5. Loop through each model name and create a new model based on the name\n",
    "    for model_name in models_list:\n",
    "        # 6. Create information print outs\n",
    "        experiment_number += 1\n",
    "        print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "        print(f\"[INFO] Model: {model_name}\")\n",
    "        print(f\"[INFO] Number of epochs: {epochs}\")\n",
    "        \n",
    "        # 7. Select the model\n",
    "        if model_name == \"effnet_b0_B-G-R-NIR\":\n",
    "            model = model.EfficientNet.from_pretrained('efficientnet-b0', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "        else:\n",
    "            model = model.EfficientNet.from_pretrained('efficientnet-b7', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "                 \n",
    "        # 8. Create a new loss and optimizer for every model\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=loss_weight)  \n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "        \n",
    "        # 9. Train target model with target dataloaders and track experiments\n",
    "        results = train(model=model,\n",
    "                        n_bands=n_bands,\n",
    "                        batch_size=batch_size,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        val_dataloader=val_dataloader, \n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        num_classes=num_classes,\n",
    "                        epochs=epochs,\n",
    "                        device=device,\n",
    "                        writer=create_writer(target_dir=target_dir,\n",
    "                                             experiment_name='test',\n",
    "                                             model_name=model_name,\n",
    "                                             extra=f\"{epochs}_epochs\"))\n",
    "        \n",
    "        # 10. Save the model to file so we can get back the best model\n",
    "        save_filepath = f\"01_{model_name}_{epochs}_epochs.pth\"\n",
    "        save_model(model=model,\n",
    "                   target_dir=\"models\",\n",
    "                   model_name=save_filepath)\n",
    "        print(\"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step2(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              num_classes: int,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_precision, test_recall, test_f1_score = 0, 0, 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            #loss_weighted = loss_fn(y_pred, y)\n",
    "            #test_loss_weighted += loss_weighted.item() \n",
    "            \n",
    "            # Calculate and accumulate evaluation metrics            \n",
    "            pre_metrics = MulticlassPrecision(num_classes=num_classes, average='weighted').to(device)\n",
    "            test_precision += pre_metrics(y_pred, y)\n",
    "            \n",
    "            rec_metrics = MulticlassRecall(num_classes=num_classes, average='weighted').to(device)\n",
    "            test_recall += rec_metrics(y_pred, y)\n",
    "            \n",
    "            f1_metrics = MulticlassF1Score(num_classes=num_classes, average='weighted').to(device)\n",
    "            test_f1_score += f1_metrics(y_pred, y)\n",
    "             \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_precision = test_precision / len(dataloader)\n",
    "    test_recall = test_recall / len(dataloader)\n",
    "    test_f1_score = test_f1_score / len(dataloader)\n",
    "    \n",
    "   \n",
    "   \n",
    "    return test_loss, test_recall, test_precision, test_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada49b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d032860",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=loss_weight)\n",
    "test_loss, test_recall, test_precision, test_f1_score = test_step2(model=model,\n",
    "                                                                         dataloader=test_dataloader,\n",
    "                                                                         loss_fn=loss_fn,\n",
    "                                                                         num_classes=num_classes,\n",
    "                                                                         device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924805a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Test recall: {test_recall}\")\n",
    "print(f\"Test precision: {test_precision}\")\n",
    "print(f\"Test F1score: {test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbc663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model: torch.nn.Module, \n",
    "                     test_dataloader: torch.utils.data.DataLoader,\n",
    "                     device: torch.device):\n",
    "    # 1. Make predictions with trained model\n",
    "    y_preds = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
    "            # Send data and targets to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Do the forward pass\n",
    "            y_logit = model(X)\n",
    "            # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
    "            y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n",
    "            # Put predictions on CPU for evaluation\n",
    "            y_preds.append(y_pred.cpu())\n",
    "    # Concatenate list of predictions into a tensor\n",
    "    y_pred_tensor = torch.cat(y_preds)\n",
    "    return y_pred_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_labels_3classes(test_dataset: torch.utils.data.dataset.Dataset):\n",
    "    torch_tensor = test_dataset.labels\n",
    "    torch_tensor = torch_tensor.squeeze(dim=1)\n",
    "    np_arr = torch_tensor.cpu().detach().numpy()\n",
    "    label_list = []\n",
    "    for i in np_arr:\n",
    "        if i <= 45:\n",
    "            label = 0\n",
    "            label_list.append(label)\n",
    "        elif i > 45 and i <= 95:\n",
    "            label = 1\n",
    "            label_list.append(label)\n",
    "        else:\n",
    "            label = 2\n",
    "            label_list.append(label)\n",
    "    array_labels = np.array(label_list)\n",
    "    val_labels = torch.from_numpy(array_labels)\n",
    "    return val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup confusion matrix instance and compare predictions to targets\n",
    "#from wze_uav.analysis import *\n",
    "y_pred_tensor = make_predictions(model=model,\n",
    "                                 test_dataloader=test_dataloader, \n",
    "                                 device=device)\n",
    "\n",
    "val_labels = val_labels_3classes(test_dataset=test_dataset)\n",
    "\n",
    "confmat = ConfusionMatrix(num_classes=num_classes, task='multiclass')\n",
    "confmat_tensor = confmat(preds=y_pred_tensor,\n",
    "                         target=val_labels)\n",
    "\n",
    "# 3. Plot the confusion matrix\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n",
    "    class_names=class_names, # turn the row and column labels into class names\n",
    "    figsize=(10, 7)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ec08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the best model filepath\n",
    "best_model_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\01_effnet_b0_train_data_100_percent_5_epochs.pth\"\n",
    "\n",
    "# Instantiate a new instance of EffNetB0 (to load the saved state_dict() to)\n",
    "unfreeze=True\n",
    "best_model = models.create_effnetb0(output_shape=num_classes, unfreeze=unfreeze, device=device)\n",
    "\n",
    "# Load the saved best model state_dict()\n",
    "best_model.load_state_dict(torch.load(best_model_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
