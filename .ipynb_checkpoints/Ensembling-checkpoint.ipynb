{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1acdaa3",
   "metadata": {},
   "source": [
    "# WZE-UAV Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2159aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0026f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b160d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wze_uav.data_loader as data_loader\n",
    "import wze_uav.models as models\n",
    "from wze_uav.engine import *\n",
    "from wze_uav.utils2 import *\n",
    "#from wze_uav.log_writer import create_writer\n",
    "from wze_uav.datasplit import *\n",
    "from efficientnet import model_effnet #for custom effnet with n_channels input\n",
    "#import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4fb2f",
   "metadata": {},
   "source": [
    "#### Get PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10886b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.13.1+cu116\n",
      "torchvision version: 0.14.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a83cf",
   "metadata": {},
   "source": [
    "#### Preparing device agnostic code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d5de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Index of current divice: 0\n",
      "Number of GPUs available: 1\n",
      "GPU Model: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "# ensure device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# get index of currently selected device\n",
    "print(f\"Index of current divice: {torch.cuda.current_device()}\")\n",
    "# get number of GPUs available\n",
    "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "# get the name of the device\n",
    "print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a6a34",
   "metadata": {},
   "source": [
    "#### Ensure reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4656a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for more information, see also: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # seed for numpy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seeds(42) \n",
    "\n",
    "# Set to true -> might speed up the process but should be set to False if reproducible results are desired\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be33671",
   "metadata": {},
   "source": [
    "#### Define file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30645f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# 3 channel input (r-g-b)\n",
    "data_path = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\Datasplit\\ROI\\rgb\"\n",
    "\n",
    "# 4 channel input (r-g-b-nir)\n",
    "#data_path = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\Datasplit\\ROI\\rgb-nir\"\n",
    "\n",
    "# 5 channel input (r-g-b-re-nir)\n",
    "#data_path = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\Datasplit\\ROI\\rgb-re-nir\"\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4560ce64",
   "metadata": {},
   "source": [
    "#### Get all file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa072e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_list = os.listdir(data_path)\n",
    "path_list = []\n",
    "# Iterate over all datafiles\n",
    "for year in fn_list:\n",
    "    year_dir = f'{data_path}\\\\{year}'\n",
    "    for filename in os.listdir(year_dir):\n",
    "        path = f'{year_dir}\\\\{filename}'\n",
    "        path_list.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3937a44",
   "metadata": {},
   "source": [
    "#### Create unique hash IDs for every individual tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a50f1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f08c08cc6a4ec9acac838152fd0943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating unique tree IDs...:   0%|          | 0/647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hashID_dict = data_loader.get_unique_treeID(path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4cea00",
   "metadata": {},
   "source": [
    "#### Import all imagery, labels and other features from hdf5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e3eecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b046b113744539a35149c4e5b5bfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing hdf5 datasets:   0%|          | 0/647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_set, label_set, species_set, kkl_set, bk_set, hash_id = data_loader.hdf5_to_img_label(path_list,\n",
    "                                                                                               hashID_dict,\n",
    "                                                                                               load_sets=[\"images_masked\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a90e2b7",
   "metadata": {},
   "source": [
    "#### Convert nbv to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0224972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = nbv_to_sst_3classes(label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4cd85",
   "metadata": {},
   "source": [
    "#### Split data into a sub set and a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06eda54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL POSITIVE RATIO: 0.4373421717171717\n",
      "Fold : 0\n",
      "TRAIN POSITIVE RATIO: 0.43524657447748943\n",
      "TEST POSITIVE RATIO : 0.4478082623777988\n",
      "LENGTH TRAIN GROUPS : 5901\n",
      "LENGTH TEST GROUPS  : 1188\n",
      "Number of True in sub_indices: 15837\n",
      "Number of False in sub_indices: 3171\n",
      "Number of True in test_indices: 1188\n",
      "Number of False in test_indices: 17820\n",
      "Check shapes:\n",
      "\n",
      "Images sub dataset: (15837, 250, 250, 3)\n",
      "Labels sub dataset: (15837, 1)\n",
      "\n",
      "Images test dataset: (1188, 250, 250, 3)\n",
      "Labels test dataset: (1188, 1)\n",
      "\n",
      "--------------------------------------------------\n",
      "Check if the split was stratified: (random_state=42)\n",
      "Healthy trees in sub dataset: 9297\n",
      "Stressed trees in sub dataset: 6187\n",
      "Dead trees in sub dataset: 353\n",
      "Healthy trees in test dataset: 672\n",
      "Stressed trees in test dataset: 491\n",
      "Dead trees in test dataset: 25\n",
      "Ratio health trees in test dataset: 0.0722813810906744\n",
      "Ratio stressed trees in test dataset: 0.07935994827864878\n",
      "Ratio dead trees in test dataset: 0.0708215297450425\n"
     ]
    }
   ],
   "source": [
    "sub_image_set, sub_label_set, sub_hash_id, sub_species_set, test_image_set, test_label_set, test_hash_id, test_species_set = data_split(image_set, label_set, hash_id, species_set, n_splits=6, random_state=42, seed=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fbb5b",
   "metadata": {},
   "source": [
    "#### Check if any hash ID is in both sub and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "874367cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no hash_id values in both train and test datasets. The datasplit was successful\n"
     ]
    }
   ],
   "source": [
    "hash_set = set(sub_hash_id[:,0].flatten())\n",
    "test_hash_set = set(test_hash_id[:,0].flatten())\n",
    "intersection = hash_set.intersection(test_hash_set)\n",
    "if intersection:\n",
    "    print(f\"Hash_id values in both train and test sets: {len(intersection)}\")\n",
    "else:\n",
    "    print(\"There are no hash_id values in both train and test datasets. The datasplit was successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b64d4",
   "metadata": {},
   "source": [
    "#### Check feature distribution of the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33ff5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset\n",
      "Test data healthy trees: 672\n",
      "Test data stressed trees: 491\n",
      "Test data dead trees: 25\n",
      "Test data pine trees: 281\n",
      "Test data spruces: 481\n",
      "--------------------------------------------------\n",
      "Remaining dataset\n",
      "Remaining data healthy trees: 9297\n",
      "Remaining data stressed trees: 6187\n",
      "Remaining data dead trees: 353\n",
      "Remaining data pine trees: 4058\n",
      "Remaining data spruces: 5863\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def count_occurrences(data, value):\n",
    "    count = 0\n",
    "    for item in data:\n",
    "        if item == value:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(\"Test dataset\")\n",
    "print(f\"Test data healthy trees: {count_occurrences(test_label_set, 0)}\")\n",
    "print(f\"Test data stressed trees: {count_occurrences(test_label_set, 1)}\")\n",
    "print(f\"Test data dead trees: {count_occurrences(test_label_set, 2)}\")\n",
    "print(f\"Test data pine trees: {count_occurrences(test_species_set, 134)}\")\n",
    "print(f\"Test data spruces: {count_occurrences(test_species_set, 118)}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Remaining dataset\")\n",
    "print(f\"Remaining data healthy trees: {count_occurrences(sub_label_set, 0)}\")\n",
    "print(f\"Remaining data stressed trees: {count_occurrences(sub_label_set, 1)}\")\n",
    "print(f\"Remaining data dead trees: {count_occurrences(sub_label_set, 2)}\")\n",
    "print(f\"Remaining data pine trees: {count_occurrences(sub_species_set, 134)}\")\n",
    "print(f\"Remaining data spruces: {count_occurrences(sub_species_set, 118)}\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e08fcf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train transform with augmentation. \n",
    "transform_train = transforms.Compose([transforms.ToTensor(), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomVerticalFlip(p=0.5),\n",
    "                                      transforms.RandomRotation(degrees=[0,360])])\n",
    "\n",
    "# test and val dataset transform without augmentation. \n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# class names need to fit the customDataset class used e.g. 3 classes -> use CustomDataset3Classes\n",
    "#class_names = ['healthy', 'slightly_stressed', 'moderately_stressed', 'highly_stressed', 'dead']\n",
    "#class_names = ['healthy', 'moderately_stressed', 'highly_stressed', 'dead']\n",
    "class_names = ['healthy', 'stressed', 'dead']\n",
    "\n",
    "# set seeds\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "NUM_WORKERS=3 # should be changed, depending on the system used\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31779c05",
   "metadata": {},
   "source": [
    "#### Define variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10ae4379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "# 1. Define number of epochs\n",
    "epochs = 50\n",
    "n_bands = sub_image_set[0].shape[2] # get number of bands\n",
    "\n",
    "# 2. Define model\n",
    "num_classes = len(class_names)\n",
    "unfreeze = True # all layer weights get updated\n",
    "dropout_rate = 0.5 #define dropout rate\n",
    "model_name = \"EffNet_b7_RGB-RE-NIR_3classes\"\n",
    "\n",
    "# 3. Define loss, optimizer and learning rate scheduler\n",
    "lr = 0.005 # define learning rate\n",
    "min_lr = 1e-6 # minimum learning rate threshold\n",
    "gamma = 0.75 # how fast the learning rate decreases per epoch (low number=faster decrease)\n",
    "patience = 10\n",
    "\n",
    "# 4. Create target folder name were to save the tensorboard event files\n",
    "experiment_name = 'RGB-RE-NIR_3classes'\n",
    "extra = \"RGB-RE-NIR_3classes\"\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "#torch.cuda.empty_cache()\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated()} bytes\") \n",
    "\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4f48946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unused variables\n",
    "del sub_image_set, sub_label_set, sub_hash_id, sub_species_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0c36ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "\n",
    "test_dataset = data_loader.CustomTestDataset(\n",
    "    data = test_image_set,\n",
    "    labels = test_label_set,\n",
    "    class_names=class_names, \n",
    "    species = test_species_set,\n",
    "    kkl = None,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# create test dataloader\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             persistent_workers=True,\n",
    "                             pin_memory=True,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=False,\n",
    "                             drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6017aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created new effnet_b7 model.\n",
      "[INFO] Created new effnet_b7 model.\n",
      "[INFO] Created new effnet_b7 model.\n",
      "[INFO] Created new effnet_b7 model.\n",
      "[INFO] Created new effnet_b7 model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f484effe343444ba9bae38a81a61c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72       669\n",
      "           1       0.61      0.59      0.60       490\n",
      "           2       0.70      0.64      0.67        25\n",
      "\n",
      "    accuracy                           0.67      1184\n",
      "   macro avg       0.67      0.65      0.66      1184\n",
      "weighted avg       0.67      0.67      0.67      1184\n",
      "\n",
      "[[487 179   3]\n",
      " [196 290   4]\n",
      " [  2   7  16]]\n"
     ]
    }
   ],
   "source": [
    "## Setup the best model filepaths\n",
    "#best_model1_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\1_EffNet_b7_RGB-RE-NIR_3classes_13_epochs.pth\"\n",
    "#best_model2_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\2_EffNet_b7_RGB-RE-NIR_3classes_14_epochs.pth\"\n",
    "#best_model3_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\3_EffNet_b7_RGB-RE-NIR_3classes_26_epochs.pth\"\n",
    "#best_model4_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\4_EffNet_b7_RGB-RE-NIR_3classes_36_epochs.pth\"\n",
    "#best_model5_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\5_EffNet_b7_RGB-RE-NIR_3classes_14_epochs.pth\"\n",
    "\n",
    "# Setup the best model filepaths\n",
    "best_model1_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB_3classes_v2\\1_EffNet_b7_RGB_3classes_v2_18_epochs.pth\"\n",
    "best_model2_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB_3classes_v2\\2_EffNet_b7_RGB_3classes_v2_28_epochs.pth\"\n",
    "best_model3_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB_3classes_v2\\3_EffNet_b7_RGB_3classes_v2_20_epochs.pth\"\n",
    "best_model4_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB_3classes_v2\\4_EffNet_b7_RGB_3classes_v2_21_epochs.pth\"\n",
    "best_model5_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB_3classes_v2\\5_EffNet_b7_RGB_3classes_v2_12_epochs.pth\"\n",
    "\n",
    "# Instantiate a new instance of EffNetB7 (to load the saved state_dict() to)\n",
    "model1 = models.create_effnetb7(output_shape=num_classes, unfreeze=unfreeze, dropout_rate=dropout_rate, device=device)\n",
    "model2 = models.create_effnetb7(output_shape=num_classes, unfreeze=unfreeze, dropout_rate=dropout_rate, device=device)\n",
    "model3 = models.create_effnetb7(output_shape=num_classes, unfreeze=unfreeze, dropout_rate=dropout_rate, device=device)\n",
    "model4 = models.create_effnetb7(output_shape=num_classes, unfreeze=unfreeze, dropout_rate=dropout_rate, device=device)\n",
    "model5 = models.create_effnetb7(output_shape=num_classes, unfreeze=unfreeze, dropout_rate=dropout_rate, device=device)\n",
    "# for custom models with more than three bands as input\n",
    "#model1 = model_effnet.EfficientNet.from_pretrained('efficientnet-b7', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "#model2 = model_effnet.EfficientNet.from_pretrained('efficientnet-b7', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "#model3 = model_effnet.EfficientNet.from_pretrained('efficientnet-b7', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "#model4 = model_effnet.EfficientNet.from_pretrained('efficientnet-b7', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "#model5 = model_effnet.EfficientNet.from_pretrained('efficientnet-b7', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "\n",
    "# Load the saved best model state_dict()\n",
    "model1.load_state_dict(torch.load(best_model1_path))\n",
    "model2.load_state_dict(torch.load(best_model2_path))\n",
    "model3.load_state_dict(torch.load(best_model3_path))\n",
    "model4.load_state_dict(torch.load(best_model4_path))\n",
    "model5.load_state_dict(torch.load(best_model5_path))\n",
    "\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)\n",
    "model4.to(device)\n",
    "model5.to(device)\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "model5.eval()\n",
    "\n",
    "# Initialize the lists to store the predictions\n",
    "all_preds_model1 = []\n",
    "all_preds_model2 = []\n",
    "all_preds_model3 = []\n",
    "all_preds_model4 = []\n",
    "all_preds_model5 = []\n",
    "\n",
    "all_true_labels = []\n",
    "\n",
    "all_species = []\n",
    "\n",
    "# Loop through the test dataset and generate predictions for each model\n",
    "with torch.inference_mode():\n",
    "    # Loop through DataLoader batches\n",
    "    for batch, (X, y, species) in tqdm(enumerate(test_dataloader)):\n",
    "        \n",
    "        X, y, species = X.to(device), y, species\n",
    "        \n",
    "        # Generate predictions for each model\n",
    "        preds_model1 = model1(X)\n",
    "        preds_model2 = model2(X)\n",
    "        preds_model3 = model3(X)\n",
    "        preds_model4 = model4(X)\n",
    "        preds_model5 = model5(X)\n",
    "\n",
    "        # Append the predictions to the corresponding list\n",
    "        all_preds_model1.append(preds_model1.cpu().numpy())\n",
    "        all_preds_model2.append(preds_model2.cpu().numpy())\n",
    "        all_preds_model3.append(preds_model3.cpu().numpy())\n",
    "        all_preds_model4.append(preds_model4.cpu().numpy())\n",
    "        all_preds_model5.append(preds_model5.cpu().numpy())\n",
    "        \n",
    "        all_true_labels.append(y)\n",
    "        \n",
    "        all_species.append(species)\n",
    "\n",
    "# Concatenate the predictions from all the models\n",
    "all_preds_model1 = np.concatenate(all_preds_model1)\n",
    "all_preds_model2 = np.concatenate(all_preds_model2)\n",
    "all_preds_model3 = np.concatenate(all_preds_model3)\n",
    "all_preds_model4 = np.concatenate(all_preds_model4)\n",
    "all_preds_model5 = np.concatenate(all_preds_model5)\n",
    "\n",
    "all_true_labels = np.concatenate(all_true_labels)\n",
    "\n",
    "all_species = np.concatenate(all_species)\n",
    "\n",
    "# Calculate the ensemble predictions\n",
    "ensemble_preds = np.mean([all_preds_model1, all_preds_model2, all_preds_model3, all_preds_model4, all_preds_model5], axis=0)\n",
    "ensemble_labels = np.argmax(ensemble_preds, axis=1)\n",
    "\n",
    "# Calculate the evaluation metrics for the ensemble model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(all_true_labels, ensemble_labels))\n",
    "print(confusion_matrix(all_true_labels, ensemble_labels))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a199e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67db5af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the best model filepath\n",
    "#best_model_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\effnet_b0\\01_18_epochs.pth\"\n",
    "#best_model_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\1_EffNet_b7_RGB-RE-NIR_3classes_13_epochs.pth\"\n",
    "#best_model_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\2_EffNet_b7_RGB-RE-NIR_3classes_14_epochs.pth\"\n",
    "#best_model_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\3_EffNet_b7_RGB-RE-NIR_3classes_26_epochs.pth\"\n",
    "#best_model_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\4_EffNet_b7_RGB-RE-NIR_3classes_36_epochs.pth\"\n",
    "best_model_path = r\"C:\\Users\\lwfeckesim\\01_PyTorch\\wze-uav\\wze-uav-master\\models\\EffNet_b7_RGB-RE-NIR_3classes\\5_EffNet_b7_RGB-RE-NIR_3classes_14_epochs.pth\"\n",
    "# Instantiate a new instance of EffNetB0 (to load the saved state_dict() to)\n",
    "#unfreeze=True\n",
    "#best_model = models.create_effnetb0(output_shape=num_classes, unfreeze=unfreeze, dropout_rate=dropout_rate, device=device)\n",
    "best_model = model_effnet.EfficientNet.from_pretrained('efficientnet-b7', in_channels=n_bands, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "# Load the saved best model state_dict()\n",
    "best_model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1d3e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def make_predictions2(model: torch.nn.Module, \n",
    "                     test_dataloader: torch.utils.data.DataLoader,\n",
    "                     device: torch.device):\n",
    "    # 1. Make predictions with trained model\n",
    "    y_preds = []\n",
    "    y_labels = []\n",
    "    species_list = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y, species in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
    "            # Send data and targets to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Do the forward pass\n",
    "            y_logit = model(X)\n",
    "            # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
    "            y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n",
    "            # Put predictions on CPU for evaluation\n",
    "            y_preds.append(y_pred.cpu())\n",
    "            y_labels.append(y.cpu())\n",
    "            species_list.append(species)\n",
    "    # Concatenate list of predictions into a tensor\n",
    "    y_pred_tensor = torch.cat(y_preds)\n",
    "    y_label_tensor = torch.cat(y_labels)\n",
    "    species_all = torch.cat(species_list)\n",
    "    \n",
    "    y_pred = y_pred_tensor.detach().cpu().numpy()\n",
    "    y_true = y_label_tensor.detach().cpu().numpy()\n",
    "    species = species_all.detach().cpu().numpy()\n",
    "    return y_pred, y_true, species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aab9e321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40760fbe174141b69b6870898b54413f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making predictions:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     healthy       0.74      0.82      0.78       285\n",
      "    stressed       0.64      0.53      0.58       173\n",
      "        dead       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.71       466\n",
      "   macro avg       0.75      0.74      0.74       466\n",
      "weighted avg       0.71      0.71      0.71       466\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 2. Setup confusion matrix instance and compare predictions to targets\n",
    "#from wze_uav.analysis import *\n",
    "y_pred, y_true, species = make_predictions2(model=best_model,\n",
    "                                 test_dataloader=test_dataloader, \n",
    "                                 device=device)\n",
    "\n",
    "pred_list = []\n",
    "label_list = []\n",
    "for i in range(len(species)):\n",
    "    if species[i] == 118:\n",
    "        pred = y_pred[i]\n",
    "        label = y_true[i]\n",
    "        pred_list.append(pred)\n",
    "        label_list.append(label)\n",
    "\n",
    "y_pred = np.asarray(pred_list)\n",
    "y_true = np.asarray(label_list)\n",
    "\n",
    "labels = np.array([0,1,2])\n",
    "confmat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# 3. Plot the confusion matrix\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat, # matplotlib likes working with NumPy \n",
    "    class_names=class_names, # turn the row and column labels into class names\n",
    "    figsize=(10, 7)\n",
    ");\n",
    "\n",
    "# add title to confusion matrix plot\n",
    "plt.title(\"Picea abies\", fontsize=16)\n",
    "\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(report)\n",
    "\n",
    "#print(f\"Test loss: {test_loss}\")\n",
    "#print(f\"Test precision: {test_precision}\")\n",
    "#print(f\"Test recall: {test_recall}\")\n",
    "#print(f\"Test F1score: {test_f1_score}\")\n",
    "#print(f\"Test Kappa: {test_kappa}\")\n",
    "#print(f\"Test Accuracy: {test_acc}\")\n",
    "#print(f\"Test Logits: {y_logit}\")\n",
    "#print(f\"Test Predictions: {y_pred}\")\n",
    "#print(f\"Test Labels: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a29b248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6da05604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab63323c8c6947feb4863f3c32e0350b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making predictions:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     healthy       0.72      0.78      0.75       682\n",
      "    stressed       0.64      0.57      0.60       483\n",
      "        dead       0.80      0.84      0.82        19\n",
      "\n",
      "    accuracy                           0.69      1184\n",
      "   macro avg       0.72      0.73      0.72      1184\n",
      "weighted avg       0.69      0.69      0.69      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_preds = []\n",
    "y_labels = []\n",
    "species_list = []\n",
    "labels = np.array([0,1,2])\n",
    "test_loss, test_precision, test_recall, test_f1_score, test_acc = 0, 0, 0, 0, 0\n",
    "count = 0\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "with torch.inference_mode():\n",
    "    for X, y, species in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
    "        # Send data and targets to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Do the forward pass\n",
    "        y_logit = best_model(X)\n",
    "        # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
    "        y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n",
    "        # Put predictions on CPU for evaluation\n",
    "        y_preds.append(y_pred.cpu())\n",
    "        y_labels.append(y.cpu())\n",
    "        species_list.append(species)\n",
    "        \n",
    "        #other metrics\n",
    "        #test_acc += ((y_pred == y).sum().item()/len(y_pred))\n",
    "        #y_pred_class = y_pred.detach().cpu().numpy() \n",
    "        #y_class = y.detach().cpu().numpy()\n",
    "        #test_precision += precision_score(y_class, y_pred_class, average='macro', zero_division=1, labels=labels)\n",
    "        #test_recall += recall_score(y_class, y_pred_class, average='macro', zero_division=1, labels=labels)\n",
    "        #test_f1_score += f1_score(y_class, y_pred_class, average='macro', zero_division=1, labels=labels)\n",
    "        \n",
    "        #if count >= 1:\n",
    "        #    y_set = torch.cat((y_set, y))\n",
    "        #    count = count + 1\n",
    "        #else:\n",
    "        #    y_set = y\n",
    "        #    count = count + 1\n",
    "        \n",
    "#test_loss = test_loss / len(test_dataloader)\n",
    "#test_precision = test_precision / len(test_dataloader)\n",
    "#test_recall = test_recall / len(test_dataloader)\n",
    "#test_f1_score = test_f1_score / len(test_dataloader)\n",
    "#test_kappa = test_kappa / len(dataloader)\n",
    "#test_acc = test_acc / len(test_dataloader)\n",
    "# Concatenate list of predictions into a tensor\n",
    "y_pred_tensor = torch.cat(y_preds)\n",
    "y_label_tensor = torch.cat(y_labels)\n",
    "#test_f1_score = f1_score(y_set.detach().cpu().numpy(), y_pred_tensor.cpu().numpy(), average='macro', zero_division=0, labels=[0,1,2])\n",
    "\n",
    "# Print classification report\n",
    "y_true = y_label_tensor.detach().cpu().numpy()\n",
    "report = classification_report(y_true, y_pred_tensor.cpu().numpy(), target_names=class_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3db355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
