{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7f3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn_list = os.listdir(data_path)\n",
    "## Initialize dictionary to store groups of files with the same name\n",
    "#file_groups = {}\n",
    "#\n",
    "## Iterate over all datafiles\n",
    "#for year in fn_list:\n",
    "#    year_dir = f'{data_path}\\\\{year}'\n",
    "#    for filename in os.listdir(year_dir):\n",
    "#        # Generate unique identifier for file based on its name\n",
    "#        identifier = hashlib.md5(filename.encode()).hexdigest()\n",
    "#\n",
    "#        # Add file to appropriate group based on identifier\n",
    "#        if identifier in file_groups:\n",
    "#            file_groups[identifier][year] = os.path.join(year_dir, filename)\n",
    "#        else:\n",
    "#            file_groups[identifier] = {year: os.path.join(year_dir, filename)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b602d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the dictionary into training and testing sets\n",
    "#train_file_groups, test_file_groups = train_test_split(list(file_groups.items()), test_size=0.15, random_state=2)\n",
    "#\n",
    "## Convert the training and testing sets back to dictionaries\n",
    "#train_file_groups = {k: v for k, v in train_file_groups}\n",
    "#test_file_groups = {k: v for k, v in test_file_groups}\n",
    "#\n",
    "## Print the number of groups in the training and testing sets\n",
    "#print(f'Number of groups in the training set: {len(train_file_groups)}')\n",
    "#print(f'Number of groups in the testing set: {len(test_file_groups)}')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b269d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create lists of hashIDs and file paths\n",
    "#hashIDs = []\n",
    "#file_paths = []\n",
    "#for hashID, files in train_file_groups.items():\n",
    "#    for year, file_path in files.items():\n",
    "#        hashIDs.append(hashID)\n",
    "#        file_paths.append(file_path)\n",
    "#\n",
    "## define the number of folds for k-fold cross-validation\n",
    "#num_folds = 5\n",
    "#\n",
    "## create the k-fold cross-validator with GroupKFold\n",
    "#kf = GroupKFold(n_splits=num_folds)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109ef9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## group files with same names from each year based on their hashID to ensure spatial seperation of train and validation data\n",
    "#for fold, (train_idx, val_idx) in enumerate(kf.split(file_paths, groups=hashIDs)): \n",
    "#    # get the train and validation file paths using the indices for each fold\n",
    "#    train_lst = []\n",
    "#    val_lst = []\n",
    "#    for i in train_idx:\n",
    "#        train_lst.append(file_paths[i])\n",
    "#    #train_image_set, train_label_set, train_species_set, train_kkl_set, train_bk_set = data_loader.hdf5_to_img_label_v2(train_lst, load_sets = [\"images_masked\"])\n",
    "#    \n",
    "#    for i in val_idx:\n",
    "#        val_lst.append(file_paths[i])\n",
    "#    #val_image_set, val_label_set, val_species_set, val_kkl_set, val_bk_set = data_loader.hdf5_to_img_label_v2(val_lst, load_sets = [\"images_masked\"])\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57fbd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_lst = []\n",
    "#for hashID, year_dict in test_file_groups.items():\n",
    "#    for year, path in year_dict.items():\n",
    "#        test_lst.append(path)\n",
    "#test_image_set, test_label_set, test_species_set, test_kkl_set, test_bk_set = data_loader.hdf5_to_img_label_v2(test_lst, load_sets = [\"images_masked\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nStacked Dataset\")\n",
    "#print(f\"\\nTrain dataset shape: {train_image_set.shape}\")\n",
    "#print(f\"Train labels shape: {train_label_set.shape}\")\n",
    "#print(\"-\"*50)\n",
    "#print(f\"Validation dataset shape: {val_image_set.shape}\")\n",
    "#print(f\"Validation labels shape: {val_label_set.shape}\")\n",
    "#print(\"-\"*50)\n",
    "#print(f\"Test dataset shape: {test_image_set.shape}\")\n",
    "#print(f\"Test labels shape: {test_label_set.shape}\")\n",
    "#print(\"-\"*50)\n",
    "#print(f\"\\nTotal image samples: {train_image_set.shape[0] + val_image_set.shape[0] + test_image_set.shape[0]}\")\n",
    "#print(f\"Total label samples: {train_label_set.shape[0] + val_label_set.shape[0] + test_label_set.shape[0]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
