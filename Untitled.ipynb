{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54194ee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no field of name tnr",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m k[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkkl\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(count)] \u001b[38;5;241m=\u001b[39m crowns_human[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterrestrial\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkkl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     55\u001b[0m b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbk\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(count)] \u001b[38;5;241m=\u001b[39m crowns_human[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterrestrial\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbk\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 56\u001b[0m tnr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtnr\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(count)] \u001b[38;5;241m=\u001b[39m \u001b[43mcrowns_human\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mterrestrial\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtnr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     57\u001b[0m enr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menr\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(count)] \u001b[38;5;241m=\u001b[39m crowns_human[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterrestrial\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     58\u001b[0m sat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msat\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(count)] \u001b[38;5;241m=\u001b[39m crowns_human[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterrestrial\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: no field of name tnr"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import h5py\n",
    "import wze_uav.data_loader as data_loader\n",
    "import wze_uav.visualization as visualization\n",
    "import wze_uav.models as models\n",
    "from wze_uav.engine import *\n",
    "from wze_uav.utils2 import *\n",
    "from wze_uav.log_writer import create_writer\n",
    "import torch\n",
    "import os\n",
    "import findatree_roi.io as io\n",
    "import findatree_roi.exporter as exporter\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "data_path = r\"D:\\Drohnendaten\\10_WZE-UAV\\Auswertung_findatree\\2020\\temp\"\n",
    "\n",
    "d = {} # empty dict for the imagery\n",
    "l = {} # empty dict for the labels (needle or leafloss)\n",
    "s = {} # empty dict for the tree species\n",
    "k = {} # empty dict for the kraftsche class (e.g. if a tree is dominant or not) \n",
    "b = {} # empty dict for the tree class\n",
    "tnr_lst = []\n",
    "enr = {}\n",
    "sat = {}\n",
    "bnr = {}\n",
    "fn_list = os.listdir(data_path)\n",
    "count = 0\n",
    "for fn in fn_list:\n",
    "    # load hdf5 data from path\n",
    "    rois, params_rois, data, params_data = exporter.load_rois_from_hdf5_v2(data_path + \"\\\\\" + fn,\n",
    "                                                                           load_sets=[\"images_masked\"]) \n",
    "    tnr = fn.split(\"_\",0)[0].split(\"r\",1)[1]\n",
    "    tnr_lst-.\n",
    "    # fetch images\n",
    "    d[\"images{0}\".format(count)] = rois[\"images_masked\"] # assigns all crown arrays from the different hdf5 files to dictionary\n",
    "    images = d[\"images\" + str(count)].transpose(3,0,1,2) # array transpose shape from (H, W, C, n) to (n, H, W, C)\n",
    "    \n",
    "    # fetch terrestrial features (such as labels and species etc.)\n",
    "    crowns_human = data['crowns_human'] #get crown info with all terrestrial features\n",
    "    l[\"labels{0}\".format(count)] = crowns_human['features']['terrestrial']['nbv'] # get stress level (0, 1, 2, 3, 4)\n",
    "    s[\"species{0}\".format(count)] = crowns_human['features']['terrestrial']['ba'] # get tree species\n",
    "    k[\"kkl{0}\".format(count)] = crowns_human['features']['terrestrial']['kkl']\n",
    "    b[\"bk{0}\".format(count)] = crowns_human['features']['terrestrial']['bk']\n",
    "    \n",
    "    enr[\"enr{0}\".format(count)] = crowns_human['features']['terrestrial']['enr']\n",
    "    sat[\"sat{0}\".format(count)] = crowns_human['features']['terrestrial']['sat']\n",
    "    bnr[\"bnr{0}\".format(count)] = crowns_human['features']['terrestrial']['bnr']\n",
    "    \n",
    "    labels = l[\"labels\" + str(count)].copy() # copy to avoid memory error\n",
    "    species = s[\"species\" + str(count)].copy()\n",
    "    kkl = k[\"kkl\" + str(count)].copy()\n",
    "    bk = b[\"bk\" + str(count)].copy()\n",
    "    \n",
    "    enr = enr[\"enr\" + str(count)].copy()\n",
    "    sat = sat[\"sat\" + str(count)].copy()\n",
    "    bnr = bnr[\"bnr\" + str(count)].copy()\n",
    "    \n",
    "    labels = labels.reshape(len(labels),1) # reshape from (samples,) to (samples,1)\n",
    "    species = species.reshape(len(species),1)\n",
    "    kkl = kkl.reshape(len(kkl),1)\n",
    "    bk = bk.reshape(len(bk),1)\n",
    "    \n",
    "    enr = enr.reshape(len(enr),1)\n",
    "    sat = sat.reshape(len(sat),1)\n",
    "    bnr = bnr.reshape(len(bnr),1)\n",
    "    \n",
    "    \n",
    "    if count >= 1:\n",
    "        image_set = np.concatenate((image_set, images), axis=0)  # concatenate all crown arrays to one image_set\n",
    "        label_set = np.concatenate((label_set, labels), axis=0)  # concatenate all crown arrays to one label_set\n",
    "        species_set = np.concatenate((species_set, species), axis=0)\n",
    "        kkl_set = np.concatenate((kkl_set, kkl), axis=0)\n",
    "        bk_set = np.concatenate((bk_set, bk), axis=0)\n",
    "        \n",
    "        enr_set = np.concatenate((enr_set, enr), axis=0)\n",
    "        sat_set = np.concatenate((sat_set, sat), axis=0)\n",
    "        bnr_set = np.concatenate((bnr_set, bnr), axis=0)\n",
    "        count = count + 1\n",
    "    else:\n",
    "        image_set = images\n",
    "        label_set = labels # define label_set with the first sets of labels\n",
    "        species_set = species\n",
    "        kkl_set = kkl\n",
    "        bk_set = bk\n",
    "       \n",
    "        enr_set = enr\n",
    "        sat_set = sat\n",
    "        bnr_set = bnr\n",
    "        count = count + 1\n",
    "\n",
    "# filter data depending on terrestrial features\n",
    "np_filter = []\n",
    "for i in range(0, len(bk_set)):\n",
    "    if kkl_set[i] > 3:\n",
    "        np_filter.append(False) \n",
    "    elif bk_set[i] <= 1:\n",
    "        np_filter.append(True)\n",
    "    elif bk_set[i] >= 320 and bk_set[i] <= 340:\n",
    "        np_filter.append(True)\n",
    "    else:\n",
    "        np_filter.append(False)\n",
    "    \n",
    "image_set = image_set[np_filter]\n",
    "label_set = label_set[np_filter]\n",
    "species_set = species_set[np_filter]\n",
    "kkl_set = kkl_set[np_filter]\n",
    "bk_set = bk_set[np_filter]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fd75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
